"""
å•ä¸ªé¥°å“æ›´æ–°å™¨
ä¸“é—¨å¤„ç†å•ä¸ªé¥°å“çš„æ•°æ®æ›´æ–°åŠŸèƒ½
"""
import asyncio
import os
import sys
import random
import subprocess
from pathlib import Path
from typing import Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from crawler.core import load_config_json, load_cookie_file, fetch_single_item_by_search, load_proxies, get_random_proxy
from crawler.database import writer_loop, get_db_config_from_dict


def is_batch_crawler_running() -> bool:
    """æ£€æŸ¥batch_crawleræ˜¯å¦æ­£åœ¨è¿è¡Œ"""
    try:
        # ä½¿ç”¨tasklistå‘½ä»¤æ£€æŸ¥Pythonè¿›ç¨‹
        result = subprocess.run(['tasklist', '/FI', 'IMAGENAME eq python.exe', '/FO', 'CSV'],
                              capture_output=True, text=True, encoding='gbk', errors='ignore')

        # æ£€æŸ¥è¾“å‡ºä¸­æ˜¯å¦åŒ…å«batch_crawler.py
        if result.stdout and 'python.exe' in result.stdout:
            # æ›´ç²¾ç¡®çš„æ£€æŸ¥ï¼šæŸ¥çœ‹æ˜¯å¦æœ‰åŒ…å«batch_crawlerçš„Pythonè¿›ç¨‹
            try:
                # ä½¿ç”¨wmicè·å–æ›´è¯¦ç»†çš„è¿›ç¨‹ä¿¡æ¯
                wmic_result = subprocess.run(['wmic', 'process', 'where', 'name="python.exe"', 'get', 'CommandLine', '/format:csv'],
                                           capture_output=True, text=True, encoding='gbk', errors='ignore')
                if wmic_result.stdout and 'batch_crawler' in wmic_result.stdout.lower():
                    print('æ£€æµ‹åˆ°batch_crawleræ­£åœ¨è¿è¡Œ')
                    return True
            except Exception as e:
                print(f'WMICæ£€æŸ¥å¤±è´¥: {e}')

        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°batch_crawlerï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ˜æ˜¾çš„çˆ¬è™«è¿›ç¨‹
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„æ£€æŸ¥é€»è¾‘ï¼Œæ¯”å¦‚æ£€æŸ¥ç‰¹å®šçš„è¿›ç¨‹åæˆ–å‚æ•°

    except Exception as e:
        print(f'æ£€æŸ¥batch_crawlerè¿›ç¨‹å¤±è´¥: {e}')

    return False


def get_random_account_cookie(cookie_files: list) -> Optional[dict]:
    """ä»å¤šä¸ªcookieæ–‡ä»¶ä¸­éšæœºé€‰æ‹©ä¸€ä¸ª"""
    if not cookie_files:
        return {}
    
    selected_file = random.choice(cookie_files)
    try:
        # åŒæ­¥åŠ è½½cookieï¼ˆè¿™é‡Œä½¿ç”¨åŒæ­¥æ–¹å¼ï¼Œå› ä¸ºæ˜¯åˆå§‹åŒ–é˜¶æ®µï¼‰
        cookies = {}
        if os.path.exists(selected_file):
            with open(selected_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line or '=' not in line:
                        continue
                    k, v = line.split('=', 1)
                    cookies[k.strip()] = v.strip()
            print(f'éšæœºé€‰æ‹©account: {selected_file} ({len(cookies)} cookies)')
            return cookies
        else:
            print(f'Cookieæ–‡ä»¶ä¸å­˜åœ¨: {selected_file}')
    except Exception as e:
        print(f'åŠ è½½cookieæ–‡ä»¶å¤±è´¥: {e}')
    
    return {}


async def update_single_item(item_id: str = None, item_name: str = None, config_path: str = None):
    """å•ä¸ªé¥°å“æ›´æ–°å‡½æ•°"""
    if not item_id and not item_name:
        print('é”™è¯¯: å¿…é¡»æä¾›é¥°å“IDæˆ–åç§°')
        return False
    
    print(f'å¼€å§‹æ›´æ–°å•ä¸ªé¥°å“: ID={item_id}, Name={item_name}')
    
    # æ£€æŸ¥batch_crawleræ˜¯å¦æ­£åœ¨è¿è¡Œ
    if is_batch_crawler_running():
        print('âŒ é”™è¯¯: æ£€æµ‹åˆ°batch_crawleræ­£åœ¨è¿è¡Œï¼')
        print('âš ï¸  ä¸ºäº†é¿å…429é”™è¯¯å’Œè´¦å·é£é™©ï¼Œå•æ¬¡æ›´æ–°ä¸æ‰¹é‡çˆ¬å–ä¸èƒ½åŒæ—¶è¿è¡Œ')
        print('è¯·ç­‰å¾…batch_crawlerå®Œæˆåå†è¿è¡Œå•æ¬¡æ›´æ–°')
        return False
    
    # ç¡®å®šé…ç½®æ–‡ä»¶è·¯å¾„
    if not config_path:
        config_path = os.path.join(project_root, 'config.json')
    
    # åŠ è½½é…ç½®
    json_config = await load_config_json(config_path)
    
    def env_str(name, json_key=None, default=None):
        v = os.environ.get(name)
        if v is not None:
            return v
        elif json_key and json_key in json_config:
            return json_config[json_key]
        else:
            return default
    
    def env_int(name, json_key=None, default=0):
        v = os.environ.get(name)
        try:
            if v is not None:
                return int(v)
            elif json_key and json_key in json_config:
                return int(json_config[json_key])
            else:
                return int(default)
        except Exception:
            return int(default)
    
    def env_bool(name, json_key=None, default=False):
        v = os.environ.get(name)
        if v is None:
            return json_config.get(json_key, default) if json_key else default
        return str(v).lower() in ('1', 'true', 'yes', 'on')
    
    CONFIG = {
        'host': env_str('BUFF_DB_HOST', None, json_config.get('db', {}).get('host', 'localhost')),
        'port': env_int('BUFF_DB_PORT', None, json_config.get('db', {}).get('port', 3306)),
        'user': env_str('BUFF_DB_USER', None, json_config.get('db', {}).get('user', 'root')),
        'password': env_str('BUFF_DB_PASSWORD', None, json_config.get('db', {}).get('password', '123456')),
        'database': env_str('BUFF_DB_DATABASE', None, json_config.get('db', {}).get('database', 'buffotte')),
        'table': env_str('BUFF_DB_TABLE', None, json_config.get('db', {}).get('table', 'items')),
        'cookie_file': env_str('BUFF_COOKIE_FILE', 'cookie_file', os.path.join(project_root, 'cookies', 'cookies.txt')),
        'enable_price_history': env_bool('BUFF_ENABLE_PRICE_HISTORY', 'enable_price_history', True),
    }
    
    # åŠ è½½æ‰€æœ‰å¯ç”¨çš„cookieæ–‡ä»¶ä½œä¸ºaccountåˆ—è¡¨
    cookie_base_path = os.path.join(project_root, 'cookies')
    cookie_files = []
    if os.path.exists(cookie_base_path):
        for file in os.listdir(cookie_base_path):
            if file.endswith('.txt'):
                cookie_files.append(os.path.join(cookie_base_path, file))
    
    if not cookie_files:
        print('è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•cookieæ–‡ä»¶ï¼Œå°†ä½¿ç”¨æ— è´¦å·æ¨¡å¼')
        cookies = {}
    else:
        # å°è¯•ä¸åŒçš„accountï¼Œç›´åˆ°æˆåŠŸæˆ–å…¨éƒ¨å°è¯•å®Œ
        success = False
        max_attempts = min(len(cookie_files), 3)  # æœ€å¤šå°è¯•3ä¸ªä¸åŒçš„account
        
        for attempt in range(max_attempts):
            # éšæœºé€‰æ‹©ä¸€ä¸ªaccount
            cookies = get_random_account_cookie(cookie_files)
            
            # åŠ è½½ä»£ç†æ± 
            proxies = []
            proxy_file = os.path.join(project_root, 'proxy_pool', 'proxy_pool_status.json')
            if os.path.exists(proxy_file):
                try:
                    proxies = await load_proxies(proxy_file)
                    print(f'åŠ è½½ä»£ç†æ± : {len(proxies)} ä¸ªä»£ç†')
                except Exception as e:
                    print(f'åŠ è½½ä»£ç†æ± å¤±è´¥: {e}')
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªä»£ç†
            selected_proxy = get_random_proxy(proxies)
            if selected_proxy:
                print(f'éšæœºé€‰æ‹©ä»£ç†: {selected_proxy}')
            else:
                print('æ— å¯ç”¨ä»£ç†ï¼Œä½¿ç”¨ç›´è¿æ¨¡å¼')
            
            # æ•°æ®åº“é…ç½®
            db_conf = get_db_config_from_dict(CONFIG)
            
            # åˆ›å»ºå¼‚æ­¥é˜Ÿåˆ—ç”¨äºæ•°æ®åº“å†™å…¥
            queue = asyncio.Queue(maxsize=100)
            
            # å¯åŠ¨æ•°æ®åº“å†™å…¥ä»»åŠ¡
            writer_task = asyncio.create_task(
                writer_loop(queue, db_conf, CONFIG.get('table'), CONFIG.get('enable_price_history', True))
            )
            
            try:
                # è·å–é¥°å“è¯¦ç»†ä¿¡æ¯ï¼ˆæš‚æ—¶ä¸æ”¯æŒä»£ç†ï¼Œå› ä¸ºhttpx AsyncClientä¸æ”¯æŒï¼‰
                item_data = await fetch_single_item_by_search(item_id, item_name, cookies)
                if item_data:
                    await queue.put(item_data)
                    print(f'æˆåŠŸè·å–é¥°å“æ•°æ®: {item_data.get("name", "æœªçŸ¥")} (Â¥{item_data.get("sell_min_price", 0)})')
                    success = True
                    break  # æˆåŠŸäº†ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                else:
                    print(f'ç¬¬{attempt + 1}æ¬¡å°è¯•è·å–é¥°å“æ•°æ®å¤±è´¥: ID={item_id}, Name={item_name}')
                    if attempt < max_attempts - 1:
                        print('å°è¯•æ›´æ¢accountå’Œproxyé‡è¯•...')
                        await asyncio.sleep(1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
            except Exception as e:
                print(f'ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥: {e}')
                if attempt < max_attempts - 1:
                    print('å°è¯•æ›´æ¢accountå’Œproxyé‡è¯•...')
                    await asyncio.sleep(1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
            finally:
                # å‘é€ç»“æŸä¿¡å·
                await queue.put(None)
                await writer_task
        
        if not success:
            print('æ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼Œå•ä¸ªé¥°å“æ›´æ–°å¤±è´¥')
            return False
    
    print('å•ä¸ªé¥°å“æ›´æ–°å®Œæˆ')
    return True


async def main():
    """å•ä¸ªé¥°å“æ›´æ–°å™¨ä¸»å…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Buffotte å•ä¸ªé¥°å“æ›´æ–°å™¨ v0.8.0')
    parser.add_argument('--item-id', type=str, help='è¦æ›´æ–°çš„é¥°å“ID')
    parser.add_argument('--item-name', type=str, help='è¦æ›´æ–°çš„é¥°å“åç§°')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    if not args.item_id and not args.item_name:
        print('âŒ é”™è¯¯: å¿…é¡»æä¾› --item-id æˆ– --item-name')
        print('ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:')
        print('  python single_item_updater.py --item-name "AK-47 | çº¢çº¿ (Field-Tested)"')
        print('  python single_item_updater.py --item-id "33815"')
        sys.exit(1)
    
    # æ£€æŸ¥batch_crawleræ˜¯å¦æ­£åœ¨è¿è¡Œ
    if is_batch_crawler_running():
        print('âŒ é”™è¯¯: æ£€æµ‹åˆ°batch_crawleræ­£åœ¨è¿è¡Œï¼')
        print('âš ï¸  ä¸ºäº†é¿å…429é”™è¯¯å’Œè´¦å·é£é™©ï¼Œå•æ¬¡æ›´æ–°ä¸æ‰¹é‡çˆ¬å–ä¸èƒ½åŒæ—¶è¿è¡Œ')
        print('è¯·ç­‰å¾…batch_crawlerå®Œæˆåå†è¿è¡Œå•æ¬¡æ›´æ–°')
        sys.exit(1)
    
    success = await update_single_item(args.item_id, args.item_name, args.config)
    if success:
        print('âœ… é¥°å“æ›´æ–°æˆåŠŸ')
        sys.exit(0)
    else:
        print('âŒ é¥°å“æ›´æ–°å¤±è´¥')
        sys.exit(1)


if __name__ == '__main__':
    asyncio.run(main())