# AIå¤šAgentåˆ†æç³»ç»Ÿ - ä½¿ç”¨ç¤ºä¾‹

## ç¤ºä¾‹1: å®Œæ•´çš„æ—¥å¸¸å·¥ä½œæµ

```powershell
# 1. è®¾ç½®API Keyï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰
$env:GEMINI_API_KEY = "your-api-key-here"

# 2. è¿è¡Œå®Œæ•´çš„æ—¥æŠ¥å‘Šï¼ˆåŒ…å«AIåˆ†æï¼‰
python -m src.run_daily_report
```

**é¢„æœŸè¾“å‡º**:
```
Inserted rows: 450
Inserted 90 rows for 2025-09-30 23:55:10 Shanghai
...

============================================================
ğŸ¤– Starting AI Multi-Agent Analysis...
============================================================

============================================================
ğŸ¤– å¯åŠ¨å¤šAgent AIåˆ†æå·¥ä½œæµ
============================================================

[1/3] ğŸ“Š æ•°æ®åˆ†æå¸ˆæ­£åœ¨åˆ†æå†å²æ•°æ®å’Œé¢„æµ‹...
âœ“ æ•°æ®åˆ†æå®Œæˆ - è¯†åˆ« 5 ä¸ªå…³é”®å‘ç°

[2/3] ğŸ“° å¸‚åœºåˆ†æå¸ˆæ­£åœ¨åˆ†æå¸‚åœºåŠ¨æ€å’Œæ–°é—»...
âœ“ å¸‚åœºåˆ†æå®Œæˆ - æƒ…ç»ª: positive

[3/3] ğŸ’¼ åŸºé‡‘ç»ç†æ­£åœ¨ç»¼åˆåˆ†æå¹¶åˆ¶å®šæŠ•èµ„ç­–ç•¥...
âœ“ æŠ•èµ„å»ºè®®ç”Ÿæˆå®Œæˆ - å»ºè®®: BUY
  ä¿¡å¿ƒåº¦: 75%

============================================================
âœ… å¤šAgentåˆ†æå·¥ä½œæµå®Œæˆ
============================================================

ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜è‡³: models/ai_analysis_report.json
ğŸ“„ HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: models/ai_analysis_report_20251002_120000.html

Email sent to your@email.com
```

## ç¤ºä¾‹2: ç‹¬ç«‹è¿è¡ŒAIåˆ†æ

```python
# test_custom_analysis.py
from llm.workflow import AnalysisWorkflow
import pandas as pd
import json

# 1. å‡†å¤‡æ•°æ®
df = pd.read_sql('SELECT * FROM kline_data_day ORDER BY timestamp DESC LIMIT 30', engine)

predictions = [
    {'day': 1, 'predicted_daily_return': 0.008, 'direction': 'up'},
    {'day': 2, 'predicted_daily_return': 0.012, 'direction': 'up'},
    {'day': 3, 'predicted_daily_return': 0.005, 'direction': 'up'},
    {'day': 4, 'predicted_daily_return': -0.003, 'direction': 'down'},
    {'day': 5, 'predicted_daily_return': 0.002, 'direction': 'up'},
]

# 2. åˆå§‹åŒ–å·¥ä½œæµ
workflow = AnalysisWorkflow(model_name='gemini-2.0-flash-exp')

# 3. è¿è¡Œåˆ†æ
results = workflow.run_full_analysis(
    historical_data=df,
    predictions=predictions,
    chart_path='models/chart.png',
    enable_news_search=False  # ä½¿ç”¨æ¨¡æ‹Ÿæ–°é—»
)

# 4. æŸ¥çœ‹ç»“æœ
print(f"æŠ•èµ„å»ºè®®: {results['summary']['recommendation']}")
print(f"ä¿¡å¿ƒåº¦: {results['summary']['confidence']*100:.0f}%")
print(f"å¸‚åœºæƒ…ç»ª: {results['summary']['market_sentiment']}")

# 5. ä¿å­˜æŠ¥å‘Š
workflow.save_results(output_dir='models')
html_path = workflow.generate_html_report(output_dir='models')
print(f"HTMLæŠ¥å‘Š: {html_path}")
```

## ç¤ºä¾‹3: åªè¿è¡Œç‰¹å®šAgent

```python
from llm.clients.gemini_client import GeminiClient
from llm.agents.data_analyst import DataAnalystAgent

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = GeminiClient(model_name='gemini-2.0-flash-exp')

# åªåˆ›å»ºæ•°æ®åˆ†æå¸ˆ
analyst = DataAnalystAgent(client=client)

# å‡†å¤‡ä¸Šä¸‹æ–‡
context = {
    'historical_data': df,
    'predictions': predictions,
    'statistics': {'current_price': 105.5, 'volatility': 0.02}
}

# è¿è¡Œåˆ†æ
result = analyst.analyze(context)

print("æ•°æ®åˆ†ææŠ¥å‘Š:")
print(result['report'])
print("\nå…³é”®å‘ç°:")
for finding in result['key_findings']:
    print(f"â€¢ {finding}")
```

## ç¤ºä¾‹4: è‡ªå®šä¹‰Agentæ¸©åº¦

```python
from llm.workflow import AnalysisWorkflow

workflow = AnalysisWorkflow()

# è°ƒæ•´å•ä¸ªAgentçš„æ¸©åº¦
workflow.data_analyst.temperature = 0.2    # æ›´ä¿å®ˆ
workflow.market_analyst.temperature = 0.7  # æ›´æœ‰åˆ›é€ æ€§
workflow.fund_manager.temperature = 0.3    # æ›´è°¨æ…

results = workflow.run_full_analysis(...)
```

## ç¤ºä¾‹5: å¸¦å›¾åƒåˆ†æçš„åŸºé‡‘ç»ç†

```python
from llm.clients.gemini_client import GeminiClient
from llm.agents.fund_manager import FundManagerAgent

client = GeminiClient()
manager = FundManagerAgent(client=client)

# åŒ…å«å›¾è¡¨è·¯å¾„çš„ä¸Šä¸‹æ–‡
context = {
    'data_analyst_report': {...},
    'market_analyst_report': {...},
    'chart_path': 'models/prediction_chart.png',  # è‡ªåŠ¨åŒ…å«å›¾åƒåˆ†æ
    'historical_data': df
}

result = manager.analyze(context)
print(result['report'])
```

## ç¤ºä¾‹6: è§£æJSONæŠ¥å‘Š

```python
import json

# è¯»å–AIåˆ†ææŠ¥å‘Š
with open('models/ai_analysis_report.json', 'r', encoding='utf-8') as f:
    report = json.load(f)

# æå–å…³é”®ä¿¡æ¯
data_report = report['data_analyst']
market_report = report['market_analyst']
final_report = report['fund_manager']

print(f"æ•°æ®åˆ†æå¸ˆå…³é”®å‘ç°:")
for finding in data_report['key_findings']:
    print(f"  â€¢ {finding}")

print(f"\nå¸‚åœºæƒ…ç»ª: {market_report['sentiment']}")
print(f"äº¤æ˜“ä¿¡å·: {market_report['signals']}")

print(f"\næœ€ç»ˆå»ºè®®: {final_report['recommendation']}")
print(f"ä¿¡å¿ƒåº¦: {final_report['confidence']*100:.0f}%")
print(f"é£é™©è¯„ä¼°: {final_report['risk_assessment']}")
```

## ç¤ºä¾‹7: æ·»åŠ è‡ªå®šä¹‰Agent

```python
# llm/agents/risk_manager.py
from llm.agents.base_agent import BaseAgent

class RiskManagerAgent(BaseAgent):
    """é£é™©ç®¡ç†ä¸“å®¶Agent"""
    
    def __init__(self, client, temperature=0.2):
        super().__init__(
            name="é£é™©ç®¡ç†å¸ˆ",
            role="Risk Manager",
            client=client,
            temperature=temperature
        )
    
    def _build_system_instruction(self):
        return """ä½ æ˜¯ä¸€åä¸“ä¸šçš„é£é™©ç®¡ç†å¸ˆï¼Œä¸“æ³¨äºè¯†åˆ«å’Œè¯„ä¼°æŠ•èµ„é£é™©ã€‚
        
ä½ çš„èŒè´£ï¼š
1. è¯†åˆ«å¸‚åœºé£é™©ã€æµåŠ¨æ€§é£é™©ã€æ”¿ç­–é£é™©
2. é‡åŒ–é£é™©æ•å£
3. åˆ¶å®šé£é™©ç¼“è§£æªæ–½
4. è®¾å®šæ­¢æŸæ­¢ç›ˆç‚¹ä½
5. æä¾›é£é™©æ§åˆ¶å»ºè®®

è¯·æä¾›ä¸“ä¸šçš„é£é™©è¯„ä¼°æŠ¥å‘Šã€‚
"""
    
    def analyze(self, context):
        # æ„å»ºé£é™©åˆ†æprompt
        data_report = context.get('data_analyst_report', '')
        market_report = context.get('market_analyst_report', '')
        
        prompt = f"""åŸºäºä»¥ä¸‹åˆ†æï¼Œè¿›è¡Œé£é™©è¯„ä¼°ï¼š

æ•°æ®åˆ†æï¼š
{data_report}

å¸‚åœºåˆ†æï¼š
{market_report}

è¯·è¯†åˆ«ä¸»è¦é£é™©å› ç´ ï¼Œå¹¶æä¾›é£é™©æ§åˆ¶å»ºè®®ã€‚
"""
        
        report = self._generate_response(prompt)
        
        return {
            'agent': self.name,
            'role': self.role,
            'report': report,
            'risk_level': self._assess_risk_level(report)
        }
    
    def _assess_risk_level(self, report):
        # ç®€å•çš„é£é™©ç­‰çº§è¯„ä¼°
        if 'é«˜é£é™©' in report or 'ä¸¥é‡' in report:
            return 'high'
        elif 'ä¸­ç­‰é£é™©' in report:
            return 'medium'
        else:
            return 'low'


# åœ¨workflowä¸­ä½¿ç”¨
from llm.workflow import AnalysisWorkflow

class CustomWorkflow(AnalysisWorkflow):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # æ·»åŠ é£é™©ç®¡ç†å¸ˆ
        self.risk_manager = RiskManagerAgent(client=self.client)
    
    def run_full_analysis(self, *args, **kwargs):
        # å…ˆè¿è¡Œæ ‡å‡†åˆ†æ
        results = super().run_full_analysis(*args, **kwargs)
        
        # å†è¿è¡Œé£é™©è¯„ä¼°
        risk_context = {
            'data_analyst_report': results['data_analyst']['report'],
            'market_analyst_report': results['market_analyst']['report']
        }
        risk_result = self.risk_manager.analyze(risk_context)
        
        # æ·»åŠ åˆ°ç»“æœä¸­
        results['risk_manager'] = risk_result
        results['summary']['risk_assessment'] = risk_result['risk_level']
        
        return results
```

## ç¤ºä¾‹8: æ‰¹é‡åˆ†æå¤šä¸ªæ—¶é—´æ®µ

```python
from datetime import datetime, timedelta
import pandas as pd

def analyze_historical_periods(days_back=90, window=30):
    """åˆ†æå†å²å¤šä¸ªæ—¶é—´æ®µï¼Œè¯„ä¼°AIå‡†ç¡®æ€§"""
    
    workflow = AnalysisWorkflow()
    results = []
    
    for i in range(0, days_back, window):
        # è·å–è¯¥æ—¶é—´æ®µçš„æ•°æ®
        end_date = datetime.now() - timedelta(days=i)
        start_date = end_date - timedelta(days=window)
        
        df = load_data_for_period(start_date, end_date)
        predictions = get_predictions_for_date(end_date)
        
        # è¿è¡Œåˆ†æ
        analysis = workflow.run_full_analysis(
            historical_data=df,
            predictions=predictions,
            chart_path=None,
            enable_news_search=False
        )
        
        results.append({
            'period': f"{start_date.date()} to {end_date.date()}",
            'recommendation': analysis['summary']['recommendation'],
            'confidence': analysis['summary']['confidence'],
            'actual_outcome': get_actual_outcome(end_date)  # éœ€è¦å®ç°
        })
    
    # åˆ†æå‡†ç¡®ç‡
    accuracy = calculate_accuracy(results)
    print(f"AIå»ºè®®å†å²å‡†ç¡®ç‡: {accuracy*100:.1f}%")
    
    return results
```

## ç¤ºä¾‹9: é›†æˆåˆ°è‡ªåŠ¨åŒ–ä»»åŠ¡

```python
# daily_automation.py
import os
import sys
from datetime import datetime

def main():
    """æ¯æ—¥è‡ªåŠ¨è¿è¡Œçš„è„šæœ¬"""
    
    print(f"[{datetime.now()}] å¼€å§‹æ¯æ—¥åˆ†æ...")
    
    # 1. æ£€æŸ¥ç¯å¢ƒ
    if not os.getenv('GEMINI_API_KEY'):
        print("é”™è¯¯: API Keyæœªè®¾ç½®")
        return 1
    
    # 2. è¿è¡Œå®Œæ•´æµç¨‹
    try:
        from src.run_daily_report import main as run_report
        run_report()
        print(f"[{datetime.now()}] åˆ†æå®Œæˆ")
        return 0
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == '__main__':
    exit(main())
```

**Windowsè®¡åˆ’ä»»åŠ¡é…ç½®**:
```powershell
# æ³¨å†Œæ¯æ—¥ä»»åŠ¡
$action = New-ScheduledTaskAction -Execute "python" -Argument "E:\github\Buffotte\daily_automation.py" -WorkingDirectory "E:\github\Buffotte"
$trigger = New-ScheduledTaskTrigger -Daily -At "23:00"
$settings = New-ScheduledTaskSettingsSet -StartWhenAvailable
Register-ScheduledTask -TaskName "Buffotte_AI_Analysis" -Action $action -Trigger $trigger -Settings $settings -User "SYSTEM"
```

## ç¤ºä¾‹10: é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
from llm.workflow import AnalysisWorkflow
import time

def run_analysis_with_retry(max_retries=3):
    """å¸¦é‡è¯•çš„åˆ†æ"""
    
    for attempt in range(max_retries):
        try:
            workflow = AnalysisWorkflow()
            results = workflow.run_full_analysis(
                historical_data=df,
                predictions=predictions,
                chart_path=chart_path
            )
            return results
            
        except Exception as e:
            print(f"å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print("æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
                return None
    
    return None
```

## æœ€ä½³å®è·µæ€»ç»“

1. **API Keyç®¡ç†**
   ```python
   # âœ… æ¨èï¼šä½¿ç”¨ç¯å¢ƒå˜é‡
   api_key = os.getenv('GEMINI_API_KEY')
   
   # âŒ ä¸æ¨èï¼šç¡¬ç¼–ç 
   api_key = "AIza..."  # å±é™©ï¼
   ```

2. **é”™è¯¯å¤„ç†**
   ```python
   # âœ… æ€»æ˜¯åŒ…å«try-except
   try:
       results = workflow.run_full_analysis(...)
   except Exception as e:
       logger.error(f"åˆ†æå¤±è´¥: {e}")
       # å›é€€åˆ°åŸºç¡€åˆ†æ
   ```

3. **æ•°æ®éªŒè¯**
   ```python
   # âœ… éªŒè¯è¾“å…¥æ•°æ®
   if df.empty or len(df) < 30:
       raise ValueError("å†å²æ•°æ®ä¸è¶³")
   
   if not predictions:
       raise ValueError("é¢„æµ‹æ•°æ®ç¼ºå¤±")
   ```

4. **ç»“æœæŒä¹…åŒ–**
   ```python
   # âœ… ä¿å­˜æ‰€æœ‰åˆ†æç»“æœ
   workflow.save_results(output_dir='models')
   workflow.generate_html_report(output_dir='models')
   ```

5. **æ€§èƒ½ä¼˜åŒ–**
   ```python
   # âœ… ç¼“å­˜é‡å¤åˆ†æ
   cache_key = f"{df.iloc[-1]['timestamp']}_{len(predictions)}"
   if cache_key in cache:
       return cache[cache_key]
   ```

---

**æ›´å¤šç¤ºä¾‹å’Œå¸®åŠ©**: 
- æŸ¥çœ‹ `llm/README.md`
- æŸ¥çœ‹æµ‹è¯•è„šæœ¬ `test_ai_analysis.py`
- æäº¤Issueæˆ–PR
